{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2547dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt\n",
    "#%pip install huggingface_hub\n",
    "#%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdac6d",
   "metadata": {},
   "source": [
    "Existem 5 tipos diferentes de mensagens:\n",
    "\n",
    "- `HumanMessage`: Isso representa uma mensagem do usuário. Geralmente consiste apenas de conteúdo.\n",
    "\n",
    "- `AIMessage`: Isso representa uma mensagem do modelo. Pode ter additional_kwargs incluídos - por exemplo, tool_calls se estiver usando chamadas de ferramentas da OpenAI.\n",
    "\n",
    "- `SystemMessage`: Isso representa uma mensagem do sistema, que indica ao modelo como se comportar. Geralmente consiste apenas de conteúdo. Nem todo modelo suporta isso.\n",
    "\n",
    "- `FunctionMessage`: Isso representa o resultado de uma chamada de função. Além do papel e conteúdo, esta mensagem tem um parâmetro de nome que transmite o nome da função que foi chamada para produzir este resultado.\n",
    "\n",
    "- `ToolMessage`: Isso representa o resultado de uma chamada de ferramenta. Isso é distinto de uma Mensagem de Função a fim de corresponder aos tipos de mensagens de função e ferramenta da OpenAI. Além do papel e conteúdo, esta mensagem tem um parâmetro tool_call_id que transmite o id da chamada à ferramenta que foi feita para produzir este resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0a7f7",
   "metadata": {},
   "source": [
    "### Importação da API do Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba79cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ab20b",
   "metadata": {},
   "source": [
    "### Configuração modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce45c39",
   "metadata": {},
   "source": [
    "### Teste de configuração do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"Você é um assistente que traduz do inglês para o francês. Traduza o seguinte.\"),\n",
    "    (\"human\", \"I love programming.\")\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc4282",
   "metadata": {},
   "source": [
    "### Chamadas simultâneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntas = [\n",
    "\t'O que é o céu?',\n",
    "\t'O que é o mar?',\n",
    "\t'Como podemos ver as estrelas?'\n",
    "]\n",
    "\n",
    "llm.batch(perguntas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e3b5f",
   "metadata": {},
   "source": [
    "### Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "\tSystemMessage(content='Você é um assistente que conta piadas.'),\n",
    "\tHumanMessage(content='Conte uma piada de um velho português')\n",
    "]\n",
    "llm.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteúdo da mensagem\n",
    "print(llm.invoke(mensagens).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metadados da mensagem\n",
    "print(llm.invoke(mensagens).response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uma maneira mais interessante de mostrar o conteúdo do chat\n",
    "mensagens = [\n",
    "\tSystemMessage(content='Você é um assistente que conta piadas.'),\n",
    "\tHumanMessage(content='Conte uma piada de um velho português')\n",
    "]\n",
    "for trecho in llm.stream(mensagens):\n",
    "\tprint(trecho.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a1cf6",
   "metadata": {},
   "source": [
    "### Prompt few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "\tHumanMessage(content='Quando é 1 + 1?'),\n",
    "\tAIMessage(content='2'),\n",
    "\tHumanMessage(content='Quando é 10 * 5?'),\n",
    "\tAIMessage(content='50'),\n",
    "\tHumanMessage(content='Quando é 10 + 3?'),\n",
    "]\n",
    "\n",
    "print(llm.invoke(mensagens).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6f486",
   "metadata": {},
   "source": [
    "### Utilização de outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "llm = HuggingFaceEndpoint(repo_id=modelo)\n",
    "chat = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b56d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
